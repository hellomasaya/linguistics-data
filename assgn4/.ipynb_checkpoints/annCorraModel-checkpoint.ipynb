{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = open('./final_train.txt', 'r')\n",
    "traininput = train_file.read()\n",
    "\n",
    "dev_file = open('./final_dev.txt', 'r')\n",
    "devinput = dev_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(input_):\n",
    "    tags = []\n",
    "    lexicons = []\n",
    "    sentences = input_.split(\"\\n\\n\")\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split(\"\\n\")\n",
    "        for word in words:\n",
    "            tokens = word.split(\"\\t\")\n",
    "            tags.append(tokens[7])\n",
    "            lexicons.append(tokens[1])\n",
    "        lexicons.append(\"</s>\")\n",
    "    \n",
    "    lexicons.pop()\n",
    "    return lexicons, tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSamples(train_words):\n",
    "    file = open(\"training_sample_window_5.txt\", \"w\")\n",
    "\n",
    "    train_data = \"\"\n",
    "    for w in train_words:\n",
    "        if w != \"</s>\" and w!= \"\" and w!= \" \":\n",
    "            train_data+=w\n",
    "            train_data+=\" \"\n",
    "        else:\n",
    "            train_data+=\"\\n\"\n",
    "    \n",
    "    training_samples = np.empty((1, 5))\n",
    "    sentences = train_data.split(\"\\n\")\n",
    "    cou = 0\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split(\" \")\n",
    "        words.pop()\n",
    "        for i in range(len(words)):\n",
    "            cou+=1\n",
    "#             print(cou)\n",
    "            lexicons=[]\n",
    "            try:\n",
    "                if i == 0:\n",
    "                    lexicons.extend([\"start\", \"start\", words[i], words[i+1], words[i+2]])        \n",
    "                elif i == 1:\n",
    "                    lexicons.extend([\"start\", words[i-1], words[i], words[i+1], words[i+2]])\n",
    "                elif i == len(words)-1:\n",
    "                    lexicons.extend([words[i-2], words[i-1], words[i], \"end\", \"end\"])\n",
    "                elif i == len(words)-2:\n",
    "                    lexicons.extend([words[i-2], words[i-1], words[i], words[i+1], \"end\"])\n",
    "                else:\n",
    "                    lexicons.extend([words[i-2], words[i-1], words[i], words[i+1], words[i+2]])\n",
    "\n",
    "\n",
    "            except:\n",
    "                if len(words) == 1:\n",
    "                    lexicons.extend([\"start\", \"start\", words[i], \"end\", \"end\"])\n",
    "                elif i==0:\n",
    "                    if len(words) == 2:\n",
    "                        lexicons.extend([\"start\", \"start\", words[i], words[i+1], \"end\"])\n",
    "                elif i==1:\n",
    "                    if len(words) == 2:\n",
    "                        lexicons.extend([\"start\", words[i-1], words[i], \"end\", \"end\"])\n",
    "                    elif len(words) == 3:\n",
    "                        lexicons.extend([\"start\", words[i-1], words[i], words[i+1], \"end\"])       \n",
    "        \n",
    "            sample = np.asarray(lexicons)\n",
    "            training_samples = np.vstack((training_samples, sample)) \n",
    "            \n",
    "    training_samples = np.delete(training_samples, 0, 0)\n",
    "    file.write(training_samples)    \n",
    "    file.close()\n",
    "    return training_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorise_using_tfidf(training_samples, test_samples, training_labels, dev_labels):\n",
    "    sample_list=[]\n",
    "    ## Preprocess before giving to tfidf\n",
    "    for sample in training_samples:\n",
    "        sample_list.append(' '.join(sample))\n",
    "    vectorizer = TfidfVectorizer(token_pattern='(?u)\\\\s?\\w+\\\\s?|\\\\s?[' + punctuation + ']\\\\s?', ngram_range=(1, 2))\n",
    "    \n",
    "    print(\"Vectorising train samples\")\n",
    "    train_matrix = vectorizer.fit_transform(sample_list)\n",
    "\n",
    "    ## Test\n",
    "    print(\"Vectorising test samples\")\n",
    "    sample_list=[]\n",
    "    for sample in test_samples:\n",
    "        sample_list.append(' '.join(sample))\n",
    "    test_matrix = vectorizer.transform(sample_list)\n",
    "    \n",
    "    test_labels = svm(train_matrix, training_labels, test_matrix)\n",
    "    \n",
    "#     check_accuracy(test_samples, test_labels)\n",
    "    \n",
    "    cm, recall, precision, f1 = calculate_cm_recall_precision(test_labels, dev_labels)\n",
    "    print(\"cm\", cm, \"\\nrecall\", recall, \"\\nprecision\", precision, \"\\nf1:\", f1)\n",
    "    print(\"meanf1:\", np.mean(f1))    \n",
    "    print(\"meanr:\", np.mean(recall))    \n",
    "    print(\"meanp:\", np.mean(precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm(training_data, training_labels, test_samples):\n",
    "    '''\n",
    "    Trains model and predicts labels for test samples\n",
    "    '''\n",
    "    clf = SVC(kernel=\"linear\")\n",
    "    print(\"TRAINING...\")\n",
    "    clf.fit(training_data, training_labels)\n",
    "    pickle.dump(model, open(\"./annCorra_model\", 'wb'))\n",
    "    return clf.predict(test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cm_recall_precision(pred_labels, true_labels):\n",
    "    y_pred = list(pred_labels)\n",
    "    y_true = true_labels\n",
    "    list1 = y_pred + y_true\n",
    "    x = np.array(list1) \n",
    "    labels = list(np.unique(x))\n",
    "#     labels = ['+','-','0','t+','t-']\n",
    "    recall = recall_score(y_true, y_pred,labels, average=None)\n",
    "    precision = precision_score(y_true, y_pred,labels, average=None)\n",
    "    f1 = f1_score(y_true, y_pred,labels, average=None)\n",
    "    return confusion_matrix(y_true, y_pred), recall, precision, f1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words, train_tags = parse(traininput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7773453e8824>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerateSamples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtraining_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-c41fbbf2a391>\u001b[0m in \u001b[0;36mgenerateSamples\u001b[0;34m(train_words)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicons\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mtraining_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtraining_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_samples = generateSamples(train_words)\n",
    "training_labels = np.asarray(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_labels.shape, training_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_words, dev_tags = parse(devinput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_samples = generateSamples(dev_words)\n",
    "dev_labels = np.asarray(dev_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dev_labels.shape, dev_samples.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"dev_sample_window_5.txt\", \"w\")\n",
    "for sample in dev_samples:\n",
    "    file.write(sample)    \n",
    "    file.write(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
